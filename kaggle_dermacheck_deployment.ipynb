{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè• DermaCheck AI - Kaggle Deployment\n",
                "## MedGemma Local Inference on T4 GPU\n",
                "\n",
                "**HAI-DEF Compliant** | **Zero Cost** | **Production Ready**\n",
                "\n",
                "---\n",
                "\n",
                "### Prerequisites Checklist\n",
                "- ‚úÖ HuggingFace access to `google/medgemma-4b-it` (approved)\n",
                "- ‚úÖ Kaggle Secrets configured (TELEGRAM_BOT_TOKEN, HF_TOKEN, NGROK_TOKEN)\n",
                "- ‚úÖ Kaggle Settings: GPU T4 + Internet ON\n",
                "\n",
                "### Expected Performance\n",
                "- Model load: ~2-3 minutes\n",
                "- Photo analysis: ~15-20 seconds\n",
                "- Text consultation: ~8-10 seconds\n",
                "- GPU memory: ~12-14GB / 16GB"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "print(\"üì¶ Installing dependencies...\\n\")\n",
                "\n",
                "# Install all required packages\n",
                "!pip install -q python-telegram-bot==21.0.0\n",
                "!pip install -q transformers>=4.40.0\n",
                "!pip install -q torch>=2.1.0 torchvision>=0.16.0\n",
                "!pip install -q bitsandbytes>=0.43.0 accelerate>=0.27.0\n",
                "!pip install -q sentencepiece protobuf\n",
                "!pip install -q pyngrok python-dotenv Pillow\n",
                "\n",
                "print(\"\\n‚úÖ All dependencies installed!\")\n",
                "print(\"üìä Package versions:\")\n",
                "!pip list | grep -E \"transformers|torch|telegram|bitsandbytes|accelerate\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîê Step 2: Setup Secrets & Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from kaggle_secrets import UserSecretsClient\n",
                "import os\n",
                "\n",
                "print(\"üîê Loading Kaggle secrets...\\n\")\n",
                "\n",
                "# Initialize secrets client\n",
                "user_secrets = UserSecretsClient()\n",
                "\n",
                "# Load secrets\n",
                "try:\n",
                "    os.environ['TELEGRAM_BOT_TOKEN'] = user_secrets.get_secret(\"TELEGRAM_BOT_TOKEN\")\n",
                "    os.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n",
                "    os.environ['NGROK_TOKEN'] = user_secrets.get_secret(\"NGROK_TOKEN\")\n",
                "    \n",
                "    print(\"‚úÖ All secrets loaded successfully!\\n\")\n",
                "    print(f\"üì± Telegram token: {os.environ['TELEGRAM_BOT_TOKEN'][:20]}...\")\n",
                "    print(f\"ü§ó HF token: {os.environ['HF_TOKEN'][:20]}...\")\n",
                "    print(f\"üåê ngrok token: {os.environ['NGROK_TOKEN'][:20]}...\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Failed to load secrets: {e}\")\n",
                "    print(\"\\nüìù Add secrets in Kaggle:\")\n",
                "    print(\"   1. Click 'Add-ons' ‚Üí 'Secrets'\")\n",
                "    print(\"   2. Add: TELEGRAM_BOT_TOKEN, HF_TOKEN, NGROK_TOKEN\")\n",
                "    print(\"   3. Enable 'Notebook access'\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÇ Step 3: Clone Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "import os\n",
                "\n",
                "# Clone DermaCheck AI repository\n",
                "REPO_URL = \"https://github.com/YOUR_USERNAME/dermacheck-ai.git\"  # ‚Üê UPDATE THIS!\n",
                "\n",
                "print(f\"üìÇ Cloning repository: {REPO_URL}\\n\")\n",
                "\n",
                "# Remove if exists\n",
                "!rm -rf dermacheck-ai\n",
                "\n",
                "# Clone\n",
                "!git clone {REPO_URL}\n",
                "\n",
                "# Change directory\n",
                "%cd dermacheck-ai\n",
                "\n",
                "print(f\"\\n‚úÖ Repository cloned successfully!\")\n",
                "print(f\"üìç Working directory: {os.getcwd()}\")\n",
                "print(\"\\nüìÅ Files:\")\n",
                "!ls -lh"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ü§ó Step 4: Authenticate HuggingFace"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "print(\"ü§ó Authenticating with HuggingFace...\\n\")\n",
                "\n",
                "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
                "\n",
                "if HF_TOKEN:\n",
                "    try:\n",
                "        login(token=HF_TOKEN)\n",
                "        print(\"‚úÖ HuggingFace authentication successful!\")\n",
                "        print(\"üîë Access granted to MedGemma models\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Authentication failed: {e}\")\n",
                "        print(\"\\n‚ö†Ô∏è  Common issues:\")\n",
                "        print(\"   1. HF access to MedGemma not approved yet\")\n",
                "        print(\"   2. Invalid token format\")\n",
                "        raise\n",
                "else:\n",
                "    print(\"‚ùå HF_TOKEN not found in secrets\")\n",
                "    raise ValueError(\"Add HF_TOKEN to Kaggle Secrets\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéÆ Step 5: Verify GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "print(\"üéÆ GPU Verification\\n\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
                "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
                "    print(f\"PyTorch Version: {torch.__version__}\")\n",
                "    print(\"\\n‚úÖ GPU ready for MedGemma!\")\n",
                "    \n",
                "    if \"T4\" in torch.cuda.get_device_name(0):\n",
                "        print(\"‚úÖ T4 detected - optimal for 4-bit quantized MedGemma\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è  GPU is {torch.cuda.get_device_name(0)} (T4 recommended)\")\n",
                "        \n",
                "else:\n",
                "    print(\"‚ùå No GPU detected!\")\n",
                "    print(\"\\nüìù Enable GPU:\")\n",
                "    print(\"   1. Kaggle Notebook Settings\")\n",
                "    print(\"   2. Accelerator ‚Üí GPU T4\")\n",
                "    print(\"   3. Save and restart kernel\")\n",
                "    raise RuntimeError(\"GPU required for MedGemma\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Step 6: Test MedGemma Model Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "print(\"üß™ Testing MedGemma model loading...\\n\")\n",
                "print(\"‚è≥ This will take ~2-3 minutes (downloads + loads model)\\n\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "from utils.model_loader import load_medgemma\n",
                "\n",
                "try:\n",
                "    # Load with 4-bit quantization\n",
                "    model, processor = load_medgemma(\n",
                "        model_name=\"google/medgemma-4b-it\",\n",
                "        quantize=True\n",
                "    )\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"‚úÖ MedGemma loaded successfully!\")\n",
                "    print(f\"üìä Model device: {model.device}\")\n",
                "    print(f\"üíæ Model dtype: {model.dtype}\")\n",
                "    \n",
                "    # Check memory usage\n",
                "    if torch.cuda.is_available():\n",
                "        allocated = torch.cuda.memory_allocated() / 1e9\n",
                "        reserved = torch.cuda.memory_reserved() / 1e9\n",
                "        print(f\"üéØ GPU Memory Allocated: {allocated:.2f} GB\")\n",
                "        print(f\"üéØ GPU Memory Reserved: {reserved:.2f} GB\")\n",
                "    \n",
                "    print(\"\\nüóëÔ∏è  Cleaning up test...\")\n",
                "    # Free memory for actual bot\n",
                "    del model\n",
                "    del processor\n",
                "    torch.cuda.empty_cache()\n",
                "    print(\"‚úÖ Test model unloaded, memory cleared\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"\\n‚ùå Model loading failed: {e}\")\n",
                "    print(\"\\nüîç Troubleshooting:\")\n",
                "    print(\"   1. Check HF access to MedGemma was approved\")\n",
                "    print(\"   2. Verify GPU is T4 (16GB VRAM)\")\n",
                "    print(\"   3. Ensure Internet is ON in Kaggle settings\")\n",
                "    print(\"   4. Try restarting the kernel\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üåê Step 7: Setup ngrok Tunnel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyngrok import ngrok\n",
                "import time\n",
                "\n",
                "print(\"üåê Setting up ngrok tunnel...\\n\")\n",
                "\n",
                "# Set ngrok auth token\n",
                "NGROK_TOKEN = os.environ.get('NGROK_TOKEN')\n",
                "ngrok.set_auth_token(NGROK_TOKEN)\n",
                "\n",
                "# Kill any existing tunnels\n",
                "ngrok.kill()\n",
                "time.sleep(2)\n",
                "\n",
                "# Start new tunnel on port 8080\n",
                "print(\"üöÄ Starting ngrok tunnel on port 8080...\")\n",
                "public_url = ngrok.connect(8080)\n",
                "\n",
                "print(f\"\\n‚úÖ ngrok tunnel active!\")\n",
                "print(f\"üåê Public URL: {public_url}\")\n",
                "\n",
                "# Save for later use\n",
                "os.environ['PUBLIC_URL'] = str(public_url)\n",
                "\n",
                "# Show active tunnels\n",
                "time.sleep(1)\n",
                "tunnels = ngrok.get_tunnels()\n",
                "print(f\"\\nüìä Active tunnels: {len(tunnels)}\")\n",
                "for tunnel in tunnels:\n",
                "    print(f\"   - {tunnel.public_url}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Step 8: RUN DERMACHECK AI BOT!\n",
                "\n",
                "**This cell will run continuously. Stop with: Kernel ‚Üí Interrupt**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Starting DermaCheck AI Bot with MedGemma!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\n‚è≥ Loading MedGemma model (2-3 minutes)...\")\n",
                "print(\"üìä Monitor progress below:\\n\")\n",
                "\n",
                "# Run the bot\n",
                "!python telegram_bot_medgemma.py\n",
                "\n",
                "# Note: This will run continuously\n",
                "# Stop with: Kernel ‚Üí Interrupt\n",
                "# Or close the notebook"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Step 9: Monitor Logs (Optional - Run Separately)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tail logs in real-time\n",
                "# Run this in a separate output if bot is running above\n",
                "!tail -f bot.log"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üßπ Cleanup (Run After Demo)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üßπ Cleaning up...\\n\")\n",
                "\n",
                "# Kill bot process\n",
                "!pkill -f telegram_bot_medgemma\n",
                "print(\"‚úÖ Bot process stopped\")\n",
                "\n",
                "# Kill ngrok\n",
                "ngrok.kill()\n",
                "print(\"‚úÖ ngrok tunnel closed\")\n",
                "\n",
                "# Clear GPU memory\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()\n",
                "    print(\"‚úÖ GPU memory cleared\")\n",
                "\n",
                "print(\"\\nüéâ Cleanup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Troubleshooting Guide\n",
                "\n",
                "### Model Loading Fails (403/404)\n",
                "**Cause**: HuggingFace access not granted yet  \n",
                "**Solution**: Wait for approval email from HuggingFace (~1-2 hours)\n",
                "\n",
                "### Out of Memory Error\n",
                "**Cause**: GPU VRAM insufficient  \n",
                "**Solution**: Verify 4-bit quantization enabled (`quantize=True`)\n",
                "\n",
                "### Bot Not Responding\n",
                "**Check**:\n",
                "1. Bot token correct (from @BotFather)\n",
                "2. ngrok tunnel active (`ngrok.get_tunnels()`)\n",
                "3. Check logs: `!tail bot.log`\n",
                "\n",
                "### GPU Not Detected\n",
                "**Solution**: Settings ‚Üí Accelerator ‚Üí GPU T4 ‚Üí Save ‚Üí Restart Kernel\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ Performance Expectations\n",
                "\n",
                "| Metric | Expected Value |\n",
                "|--------|---------------|\n",
                "| Model load | 2-3 minutes |\n",
                "| Photo analysis | 15-20 seconds |\n",
                "| Text consultation | 8-10 seconds |\n",
                "| GPU memory usage | 12-14 GB |\n",
                "| Concurrent users | 1-3 (demo) |\n",
                "\n",
                "---\n",
                "\n",
                "**Status**: Production Ready! üéâ  \n",
                "**Compliance**: HAI-DEF ‚úÖ  \n",
                "**Cost**: $0 ‚úÖ"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}